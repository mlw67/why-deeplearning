# 深度学习理论学习笔记 - 内容框架大纲

## 系列主题
**探索深度学习的理论基础：从神经网络到大模型的演进与未来**

## 核心问题
- 为什么神经网络从90年代机器学习的边缘技术演变为当今主流？
- 大模型架构为什么是有效的？为什么是这样的结构？
- 还有哪些潜在的框架结构值得探索？

> **注意**：本系列笔记不讲深度学习的数学基础知识（如线性代数、微积分、概率论等），假设读者已具备这些基础。

---

## 目录结构

### 第一章：引言与历史背景
- [1.1 神经网络的起源与发展历程](chapters/01-introduction.md)
- 1.2 从感知机到深度学习的理论演进
- 1.3 为什么深度学习在21世纪崛起
- 1.4 本系列笔记的目标与范围

### 第二章：宽度与深度理论
- [2.1 万能近似定理与宽度](chapters/02-width-depth.md)
- 2.2 深度的表达能力优势
- 2.3 宽度与深度的权衡
- 2.4 无限宽度极限与均场理论

### 第三章：神经切线核(NTK)理论
- [3.1 NTK的起源与基本概念](chapters/03-ntk.md)
- 3.2 无限宽度网络的核方法等价
- 3.3 NTK的训练动力学
- 3.4 NTK的局限性与超越

### 第四章：JL引理与维度嵌入理论
- [4.1 Johnson-Lindenstrauss引理](chapters/04-jl-embedding.md)
- 4.2 随机投影与维度缩减
- 4.3 深度学习中的隐式维度嵌入
- 4.4 注意力机制的几何视角

### 第五章：SVM与Transformer的等价性
- [5.1 支持向量机回顾](chapters/05-svm-transformer.md)
- 5.2 注意力机制的核方法解释
- 5.3 Transformer作为核机器
- 5.4 理论等价性的实践意义

### 第六章：大模型架构的理论基础
- [6.1 为什么是Transformer？](chapters/06-large-models.md)
- 6.2 规模定律(Scaling Laws)的理论解释
- 6.3 涌现能力的理论探讨
- 6.4 上下文学习(In-Context Learning)的机制

### 第七章：未来展望与潜在框架
- [7.1 当前架构的理论局限](chapters/07-future.md)
- 7.2 状态空间模型(SSM)与Mamba
- 7.3 混合专家(MoE)的理论分析
- 7.4 其他潜在框架与研究方向

---

## 参考文献与延伸阅读
每章末尾都会提供相关的原始论文和延伸阅读材料。

## 符号约定
- $\mathbf{x}$：输入向量
- $\mathbf{W}$：权重矩阵
- $f(\cdot)$：激活函数
- $\mathcal{L}$：损失函数
- $\nabla$：梯度算子
- $\mathcal{O}(\cdot)$：大O复杂度表示

## 如何使用本笔记
1. 建议按顺序阅读，各章内容有逻辑递进关系
2. 每章包含理论推导和直观解释
3. 重要定理会给出证明思路或参考文献
4. 鼓励读者结合原始论文深入学习
