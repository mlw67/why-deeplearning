# why-deeplearning

> 探索深度学习的理论基础：从神经网络到大模型的演进与未来

我们知道，在90年代神经网络只是机器学习很小的一部分，为什么经过近30年的发展，深度学习或者更小化范围说大模型成为了一种主流？这是一系列介绍深度学习理论的学习笔记，期望通过梳理深度学习的理论发展和相关研究，探讨大模型框架的有效性（为什么是这样的结构），以及可能还有哪些潜在的框架结构。

> **注意**：本系列笔记不讲深度学习的数学基础知识（如线性代数、微积分、概率论等），假设读者已具备这些基础。

## 📚 目录

详细内容框架请参阅 [OUTLINE.md](OUTLINE.md)

### 第一章：[引言与历史背景](chapters/01-introduction.md)
- 神经网络的起源与发展历程
- 从感知机到深度学习的理论演进
- 为什么深度学习在21世纪崛起

### 第二章：[宽度与深度理论](chapters/02-width-depth.md)
- 万能近似定理与宽度
- 深度的表达能力优势
- 宽度与深度的权衡
- 无限宽度极限与均场理论

### 第三章：[神经切线核(NTK)理论](chapters/03-ntk.md)
- NTK的起源与基本概念
- 无限宽度网络的核方法等价
- NTK的训练动力学
- NTK的局限性与超越

### 第四章：[JL引理与维度嵌入理论](chapters/04-jl-embedding.md)
- Johnson-Lindenstrauss引理
- 随机投影与维度缩减
- 深度学习中的隐式维度嵌入
- 注意力机制的几何视角

### 第五章：[SVM与Transformer的等价性](chapters/05-svm-transformer.md)
- 支持向量机回顾
- 注意力机制的核方法解释
- Transformer作为核机器
- 理论等价性的实践意义

### 第六章：[大模型架构的理论基础](chapters/06-large-models.md)
- 为什么是Transformer？
- 规模定律(Scaling Laws)的理论解释
- 涌现能力的理论探讨
- 上下文学习(In-Context Learning)的机制

### 第七章：[未来展望与潜在框架](chapters/07-future.md)
- 当前架构的理论局限
- 状态空间模型(SSM)与Mamba
- 混合专家(MoE)的理论分析
- 其他潜在框架与研究方向

## 🎯 核心问题

本系列笔记试图回答以下问题：

1. **架构有效性**：为什么Transformer等架构如此成功？
2. **理论基础**：这些架构背后有什么数学原理支撑？
3. **未来方向**：还有哪些潜在的架构值得探索？

## 📖 如何阅读

1. 建议按顺序阅读，各章内容有逻辑递进关系
2. 每章包含理论推导和直观解释
3. 重要定理会给出证明思路或参考文献
4. 鼓励读者结合原始论文深入学习

## 📝 License

Apache License 2.0
